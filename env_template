# =============================================================================
# OpenAI Configuration
# =============================================================================

# OpenAI Model Configuration
# Default model used for LLM interactions
OPENAI_MODEL="DeepSeek-V3.1-Terminus"

# OpenAI API Key
# Required for accessing OpenAI-compatible API endpoints
OPENAI_API_KEY=""

# OpenAI API Base URL
# Base URL for OpenAI-compatible API services
OPENAI_API_BASE="https://openapi.coreshub.cn/v1"


# =============================================================================
# Multi-Model Configuration
# =============================================================================

# Multiple Model Configuration
# Format: Multiple model configurations separated by semicolons (';')
# Each model: key-value pairs separated by commas (',')
# Example: api_key=aaa,api_base=bbb;api_key=ccc,api_base=ddd
# Purpose: Allows using multiple LLM providers with automatic failover
MODEL_SETTINGS=""


# =============================================================================
# Application Settings
# =============================================================================

# Debug Mode
# 0 = Production mode (disabled)
# 1 = Debug mode (enabled) - enables verbose logging and debugging features
DEBUG=1

DEBUG_PRINT_TRUNCATE_LENGTH=3000

# Input Messages
# CHAT_MULTI_LINE=0

# =============================================================================
# LLM Parameters
# =============================================================================

# Maximum Tokens
# Maximum number of tokens to generate in each LLM response
MAX_TOKENS=8000

# Temperature
# Controls randomness: 0.0 = deterministic, 1.0 = creative
TEMPERATURE=0.3

# Top P (Commented out - available for advanced configuration)
# Controls diversity via nucleus sampling
#TOP_P=0.97

# Frequency Penalty (Commented out - available for advanced configuration)
# Penalizes repeated tokens: 0.0 = no penalty, 1.0 = maximum penalty
#FREQUENCY_PENALTY=0.3

# =============================================================================
# System Prompt
# =============================================================================

# one file or content. Extend system prompts.
# SYSTEM_PROMPT=""

# one file or content. Describe the current environmental information.
# ENV_PROMPT=""

# Story prompt will be used when generating stories based on historical messages.
# STORY_PROMPT=""

# =============================================================================
# System Prompt Control
# =============================================================================

# Pure system prompt will only have the working mode. 1 for enabled.
PURE_SYSTEM_PROMPT=0

# =============================================================================
# Context Management
# =============================================================================

# Session Management
# SESSION_ID=
# When set, enables session-based chat history persistence
# Example usage:
#   SESSION_ID=001 llm_chat "your message here"
#   ai_list_sessions - lists all available sessions
#   ai_retrieve_messages 001 - retrieves messages for session 001

# Chat History Managers
# Format: Manager classes separated by semicolons (';')
# Each manager: class_name parameters separated by spaces (' ')
# If not set, context management is disabled
# Current implementation supports SQLAlchemy-based storage
CONTEXT_HISTORY_MANAGERS="sql.ChatHistorySQLAlchemy conn=sqlite:///memory.db;"

# Message Slimming Threshold
# Messages longer than this threshold (in tokens) will be automatically slimmed
# to optimize context window usage and prevent token overflow
CONTEXT_MESSAGES_SLIM_THRESHOLD_LENGTH=43


# =============================================================================
# Feature Flags
# =============================================================================

# Dump Messages Flag
# When set to 1, chat history messages are automatically dumped to a file
# after each execution for debugging and analysis purposes
FLAG_DUMP_MESSAGES=0


# =============================================================================
# Tool Configuration
# =============================================================================

# Give 'tools' to chat, 0 for tool_prompt only.
USE_TOOL_CALLS=0

# Disabled Tools
# Internal tools to disable (tools starting with these keywords)
# Format: Tool names separated by semicolons (';')
# Example: "agent_tool;story_tool" - disables all tools starting with these prefixes
DISABLED_TOOLS=""

# Enabled Tools
# Internal tools to enable (tools starting with these keywords)
# Format: Tool names separated by semicolons (';')
# Takes precedence over DISABLED_TOOLS - only tools matching these prefixes will be available
ENABLED_TOOLS=""

# Extra Tools
# Additional external tools to include in the agent's toolset
# Format: File paths separated by semicolons (';')
# Each tool should be defined in a separate Markdown (.md) file
EXTRA_TOOLS=""

# Plugin Tools
# External plugin tools to dynamically load
# Format: Plugin directory paths separated by semicolons (';')
# Each plugin is loaded from a separate directory and integrated via the 'TOOLS' variable
PLUGIN_TOOLS=""


# =============================================================================
# Sandbox Configuration
# =============================================================================

# Sandbox Environment Settings
# Format: Multiple sandbox configurations separated by semicolons (';')
# Each sandbox: key-value pairs separated by commas (',')
# Example: tag=ai,protocol=ssh,node=hostname;tag=ai,protocol=docker,node=host,name=container
# Supported protocols: ssh, docker
# Purpose: Enables command execution in isolated environments
SANDBOX_SETTINGS=""
